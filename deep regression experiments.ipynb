{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-eb3c42ca5825>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# number of input columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mn_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m# split into train test sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.33\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib\n",
    "\n",
    "import tensorflow as tf \n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "# define dataset\n",
    "#X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "# number of input columns\n",
    "n_inputs = X.shape[1]\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# scale data\n",
    "t = MinMaxScaler()\n",
    "t.fit(X_train)\n",
    "X_train = t.transform(X_train)\n",
    "X_test = t.transform(X_test)\n",
    "# define encoder\n",
    "visible = Input(shape=(n_inputs,))\n",
    "e = Dense(n_inputs*2)(visible)\n",
    "e = BatchNormalization()(e)\n",
    "e = ReLU()(e)\n",
    "# define bottleneck\n",
    "n_bottleneck = n_inputs\n",
    "bottleneck = Dense(n_bottleneck)(e)\n",
    "# define decoder\n",
    "d = Dense(n_inputs*2)(bottleneck)\n",
    "d = BatchNormalization()(d)\n",
    "d = ReLU()(d)\n",
    "\n",
    "# output layer\n",
    "output = Dense(n_inputs, activation='linear')(d)\n",
    "# define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# plot the autoencoder\n",
    "plot_model(model, 'autoencoder.png', show_shapes=True)\n",
    "# fit the autoencoder model to reconstruct input\n",
    "history = model.fit(X_train, X_train, epochs=400, batch_size=16, verbose=2, validation_data=(X_test,X_test))\n",
    "\n",
    "# plot loss\n",
    "#plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='test')\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "# define an encoder model (without the decoder)\n",
    "\n",
    "encoder = Model(inputs=visible, outputs=bottleneck)\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(encoder)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('encoder_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "\n",
    "plot_model(encoder, 'encoder.png', show_shapes=True)\n",
    "# save the encoder to file\n",
    "encoder.save('model\\\\encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07115929080016616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karm_ch\\AppData\\Local\\Continuum\\anaconda3\\envs\\gdal_env\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import load_model\n",
    "# define dataset\n",
    "#X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# reshape target variables so that we can transform them\n",
    "y_train = y_train.reshape((len(y_train), 1))\n",
    "y_test = y_test.reshape((len(y_test), 1))\n",
    "# scale input data\n",
    "trans_in = MinMaxScaler()\n",
    "trans_in.fit(X_train)\n",
    "X_train = trans_in.transform(X_train)\n",
    "X_test = trans_in.transform(X_test)\n",
    "# scale output data\n",
    "trans_out = MinMaxScaler()\n",
    "trans_out.fit(y_train)\n",
    "y_train = trans_out.transform(y_train)\n",
    "y_test = trans_out.transform(y_test)\n",
    "# load the model from file\n",
    "\n",
    "#encoder =tf.lite.TFLiteConverter.from_keras_model(export_dir)\n",
    "\n",
    "\n",
    "#encoder = load_model('encoder.h5')\n",
    "# encode the train data\n",
    "X_train_encode = encoder.predict(X_train)\n",
    "# encode the test data\n",
    "X_test_encode = encoder.predict(X_test)\n",
    "# define model\n",
    "model = SVR()\n",
    "# fit model on the training dataset\n",
    "model.fit(X_train_encode, y_train)\n",
    "# make prediction on test set\n",
    "yhat = model.predict(X_test_encode)\n",
    "# invert transforms so we can calculate errors\n",
    "yhat = yhat.reshape((len(yhat), 1))\n",
    "\n",
    "yhat = trans_out.inverse_transform(yhat)\n",
    "y_test = trans_out.inverse_transform(y_test)\n",
    "# calculate error\n",
    "score = mean_absolute_error(y_test, yhat)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with a multilayer perceptron regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompareRegressors(datapath): \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    from sklearn.neural_network import MLPRegressor    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    \n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from sklearn.gaussian_process.kernels import ExpSineSquared\n",
    "    from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "    from sklearn.gaussian_process.kernels import ConstantKernel\n",
    "    from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "        \n",
    "    df=pd.read_excel(datapath)\n",
    "    #C:\\articles\\to_upload\\Vizcarra\n",
    "    #print(df)\n",
    "\n",
    "    X_raw = (df[\"S2_NDVI\"].to_numpy())\n",
    "    #print(X_raw.shape)\n",
    "    X=np.reshape(X_raw,(X_raw.shape[0], 1))\n",
    "    y=(df[\"drone_NDVI\"].to_numpy()).ravel()\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
    "    regresssion_models=[]\n",
    "    \n",
    "    r_score=0\n",
    "    \n",
    "    ############################## MLPR ###################################################\n",
    "    for activation_function in (\"identity\", \"logistic\", \"tanh\", \"relu\"):\n",
    "        \n",
    "        reg = MLPRegressor(random_state=1, max_iter=500,activation=activation_function).fit(X_train, Y_train)\n",
    "        regresssion_models.append((\"MLP\",activation_function,reg,reg.score(X_test, Y_test)))\n",
    "    \n",
    "    \n",
    "    ########################  \"BOOSTED REGRESSION TREES\"  ###############################\n",
    "    for n_estimators in (20,50,100,200,300,400,500):\n",
    "    \n",
    "        reg = GradientBoostingRegressor(n_estimators=n_estimators,\n",
    "                                    max_depth=3,\n",
    "                                    learning_rate=0.1,\n",
    "                                    min_samples_split=3)\n",
    "        reg.fit(X_train, Y_train)\n",
    "        regresssion_models.append((\"BOOSTED REGRESSION TREES\",n_estimators, reg, reg.score(X_test, Y_test)))\n",
    "        \n",
    "        \n",
    "    ##################\"Gaussian Process Regressor\"############################\n",
    "    kernel_types=[]\n",
    "    kernel1 = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\n",
    "    kernel2= 1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1, alpha_bounds=(1e-5, 1e15))\n",
    "    kernel3= 1.0 * ExpSineSquared(length_scale=1.0,periodicity=3.0,length_scale_bounds=(0.1, 10.0),periodicity_bounds=(1.0, 10.0),)\n",
    "    kernel4 = DotProduct() + WhiteKernel()\n",
    "    \n",
    "    kernel_types.append(kernel1)\n",
    "    kernel_types.append(kernel2)\n",
    "    kernel_types.append(kernel3)\n",
    "    kernel_types.append(kernel4)\n",
    "    \n",
    "    for kernel in kernel_types:\n",
    "    \n",
    "        reg = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "        reg.fit(X_train, Y_train)\n",
    "        # R^2 score for the model (on the test data)\n",
    "        regresssion_models.append((\"Gaussian Process Regressor\",kernel, reg, reg.score(X_test, Y_test)))\n",
    "        \n",
    "    print(len(regression_models))\n",
    "        \n",
    "    ########################now, save the best model ##############################\n",
    "    \n",
    "    import joblib\n",
    "    max_r_score=0\n",
    "    for i in range(len(regression_models)):\n",
    "        print(regression_models[i][3])\n",
    "        model_r=float(regression_models[i][3])\n",
    "        if(model_r > max_r_score):\n",
    "            max_r_score=model_r\n",
    "        \n",
    "        \n",
    "    for k in range(len(regression_models)): \n",
    "        if(regression_models[k][3] == max_r_score):\n",
    "            # this is our best model\n",
    "            print(regression_models[k])\n",
    "            joblib.dump(regression_models[k][2],\"best_regr_model.jl\")\n",
    "            break\n",
    "        \n",
    "    return regresssion_models[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "0.7948526650519683\n",
      "-0.5925448634435388\n",
      "0.7869562619389425\n",
      "0.678582293665494\n",
      "0.9203309220632988\n",
      "0.9242615812785122\n",
      "0.9018344483293508\n",
      "0.8914942888921626\n",
      "0.8897262630495237\n",
      "0.8891312580453189\n",
      "0.8889003191495918\n",
      "0.9418523209364604\n",
      "0.7958215990507918\n",
      "0.6455117591573742\n",
      "0.9121056644932372\n",
      "('Gaussian Process Regressor', 1**2 * RBF(length_scale=1), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "                         kernel=1**2 * RBF(length_scale=1),\n",
      "                         n_restarts_optimizer=0, normalize_y=False,\n",
      "                         optimizer='fmin_l_bfgs_b', random_state=0), 0.9418523209364604)\n"
     ]
    }
   ],
   "source": [
    "datapath='C:\\\\articles\\\\to_upload\\\\with_soil_Viz_MB.xlsx'\n",
    "\n",
    "regression_models=CompareRegressors(datapath)\n",
    "#len(regression_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9530842281978945"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_models[:][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "<class 'str'>\n",
      "<class 'sklearn.gaussian_process.kernels.Sum'>\n",
      "<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(len(regression_models))\n",
    "for i in range (len(regression_models)):\n",
    "    print(type(regression_models[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-604eca5cc0a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mregression_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "regression_models[3][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_excel(\"C:\\\\articles\\\\to_upload\\\\Vizcarra\\\\Vizcarra_no_soil_grouped_NDVI.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-f5acc559a686>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgaussian_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExpSineSquared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgaussian_process\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGaussianProcessRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m kernel = 1.0 * ExpSineSquared(\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.pyplot'"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "kernel = 1.0 * ExpSineSquared(\n",
    "    length_scale=1.0,\n",
    "    periodicity=3.0,\n",
    "    length_scale_bounds=(0.1, 10.0),\n",
    "    periodicity_bounds=(1.0, 10.0),\n",
    ")\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "\n",
    "#fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n",
    "\n",
    "# plot prior\n",
    "#plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\n",
    "#axs[0].set_title(\"Samples from prior distribution\")\n",
    "\n",
    "# plot posterior\n",
    "gpr.fit(X_train, y_train)\n",
    "#plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\n",
    "#axs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\n",
    "#axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\n",
    "#axs[1].set_title(\"Samples from posterior distribution\")\n",
    "\n",
    "#fig.suptitle(\"Periodic kernel\", fontsize=18)\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
